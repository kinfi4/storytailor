{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2e8c9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from enum import Enum\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "class StoryFlavor(str, Enum):\n",
    "    FAIRY_TALE = \"fairy_tale\"\n",
    "    THRILLER = \"thriller\"\n",
    "    ROMANCE = \"romance\"\n",
    "    SCIENCE_FICTION = \"science_fiction\"\n",
    "\n",
    "\n",
    "class StoryStatus(str, Enum):\n",
    "    GENERATING_STORY = \"generating_story\"\n",
    "    COMPLETED = \"completed\"\n",
    "    GENERATING_AUDIO = \"generating_audio\"\n",
    "    FAILED = \"failed\"\n",
    "    RESTRICTED_CONTENT_DETECTED = \"restricted_content_detected\"\n",
    "    JUST_CREATED = \"just_created\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Story:\n",
    "    id: str\n",
    "    flavor: StoryFlavor\n",
    "    title: str\n",
    "    story_text: str\n",
    "    created_at: datetime\n",
    "    status: StoryStatus = StoryStatus.GENERATING_STORY\n",
    "    image_url: Optional[str] = None\n",
    "    audio_url: Optional[str] = None\n",
    "    audio_duration_seconds: Optional[float] = None\n",
    "    error_message: Optional[str] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7aa9c8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from typing import Optional, List\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class StoryGenerationRequest(BaseModel):\n",
    "    flavor: StoryFlavor\n",
    "    additional_context: Optional[str] = Field(\n",
    "        default=None,\n",
    "        alias=\"additionalContext\",\n",
    "        description=\"Additional instructions or context for the story\",\n",
    "    )\n",
    "    eighting_plus_enabled: bool = Field(\n",
    "        default=False,\n",
    "        alias=\"eightingPlusEnabled\",\n",
    "        description=\"Whether to allow 18+ content\",\n",
    "    )\n",
    "\n",
    "\n",
    "class ImageInsights(BaseModel):\n",
    "    title: str = Field(\n",
    "        ..., \n",
    "        description=\"A short title for the future story.\",\n",
    "    )\n",
    "    caption: str = Field(\n",
    "        ..., \n",
    "        description=\"A detailed description of the scene, including details.\",\n",
    "    )\n",
    "    subjects: list[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"Main visible entities (short noun phrases).\",\n",
    "    )\n",
    "    setting: str | None = Field(\n",
    "        default=None,\n",
    "        description=\"Concise environment description (e.g., 'snowy forest at dusk').\",\n",
    "    )\n",
    "    actions: list[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"Observable actions (e.g., 'walking', 'reaching for door').\",\n",
    "    )\n",
    "    mood: list[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"1-3 word mood descriptors (e.g., 'mysterious', 'cozy').\",\n",
    "    )\n",
    "    hooks: list[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"2-5 short, grounded narrative hooks to start a story.\",\n",
    "    )\n",
    "\n",
    "\n",
    "class RestrictedContentResponse(BaseModel):\n",
    "    reasoning: str = Field(\n",
    "        ...,\n",
    "        description=\"A detailed reasoning for the decision.\",\n",
    "    )\n",
    "    summary: str | None = Field(\n",
    "        default=None,\n",
    "        description=\"One-sentence rationale grounded in visible/explicit cues.\",\n",
    "    )\n",
    "    is_restricted: bool = Field(\n",
    "        default=False,\n",
    "        description=\"True if content should be blocked for under-18.\",\n",
    "    )\n",
    "\n",
    "\n",
    "class StoryGenerationResponse(BaseModel):\n",
    "    title: str = Field(..., description=\"The title of the story.\")\n",
    "    text: str = Field(..., description=\"The text of the story.\")\n",
    "\n",
    "\n",
    "class RestrictedContentDetected(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "571c6ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_WPM = 150\n",
    "\n",
    "flavour_to_wpm: dict[StoryFlavor, float] = {\n",
    "    StoryFlavor.FAIRY_TALE: 0.95 * BASE_WPM,\n",
    "    StoryFlavor.THRILLER: 0.90 * BASE_WPM,\n",
    "    StoryFlavor.ROMANCE: 0.95 * BASE_WPM,\n",
    "    StoryFlavor.SCIENCE_FICTION: 0.98 * BASE_WPM,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "700d4a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "from typing import cast\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "from PIL import Image\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "\n",
    "class StoryGenerator:\n",
    "    def __init__(self) -> None:\n",
    "        self._vision_model_name = os.getenv(\"OLLAMA_VLM_MODEL\", \"qwen2.5vl:7b\")\n",
    "        self._txt_model_name = os.getenv(\"OLLAMA_TXT_MODEL\", \"qwen2.5:7b\")\n",
    "        self._ollama_url = os.getenv(\"OLLAMA_URL\", \"http://localhost:11434\")\n",
    "        self._debug_cache_name: str | None = None\n",
    "\n",
    "    async def generate(\n",
    "        self,\n",
    "        request: StoryGenerationRequest,\n",
    "        image_bytes: bytes,\n",
    "        cache_name: str,\n",
    "    ) -> StoryGenerationResponse:\n",
    "        t0 = perf_counter()\n",
    "        image_bytes = self._convert_image_to_jpeg(image_bytes)\n",
    "        self._debug_cache_name = cache_name\n",
    "\n",
    "        t_elder = 0.0\n",
    "        if not request.eighting_plus_enabled:\n",
    "            t1 = perf_counter()\n",
    "            await self._perform_elder_content_check(request, image_bytes)\n",
    "            t_elder = perf_counter() - t1\n",
    "\n",
    "        t2 = perf_counter()\n",
    "        insights = await self._get_image_insights(request, image_bytes)\n",
    "        t_insights = perf_counter() - t2\n",
    "\n",
    "        t3 = perf_counter()\n",
    "        result = await self._generate_story(request, insights)\n",
    "        t_story = perf_counter() - t3\n",
    "\n",
    "        total = perf_counter() - t0\n",
    "        try:\n",
    "            word_count = len(result.text.split())\n",
    "        except Exception:\n",
    "            word_count = 0\n",
    "\n",
    "        self._cache_set(\"timings\", {\n",
    "            \"elder_check\": t_elder,\n",
    "            \"image_insights\": t_insights,\n",
    "            \"story\": t_story,\n",
    "            \"total\": total,\n",
    "        })\n",
    "        self._cache_set(\"story_stats\", {\n",
    "            \"word_count\": word_count,\n",
    "        })\n",
    "\n",
    "        return result\n",
    "    \n",
    "    # ---------------------\n",
    "    # Tiny debug-cache helpers (single file)\n",
    "    # ---------------------\n",
    "    def _cache_path(self) -> Path:\n",
    "        base_dir = Path.cwd().parent / \"tests\" / \"data\"\n",
    "        base_dir.mkdir(parents=True, exist_ok=True)\n",
    "        return base_dir / \"predictions.json\"\n",
    "\n",
    "    def _cache_get(self, step: str):\n",
    "        path = self._cache_path()\n",
    "        if path.exists():\n",
    "            try:\n",
    "                with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "                    data = json.load(f)\n",
    "                obj = data.get(self._debug_cache_name or \"\")\n",
    "                if isinstance(obj, dict):\n",
    "                    return obj.get(step)\n",
    "            except Exception:\n",
    "                return None\n",
    "        return None\n",
    "\n",
    "    def _cache_set(self, step: str, value) -> None:\n",
    "        path = self._cache_path()\n",
    "        blob = {}\n",
    "        if path.exists():\n",
    "            try:\n",
    "                with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "                    blob = json.load(f)\n",
    "            except Exception:\n",
    "                blob = {}\n",
    "        key = self._debug_cache_name or \"\"\n",
    "        if key not in blob or not isinstance(blob.get(key), dict):\n",
    "            blob[key] = {}\n",
    "        blob[key][step] = value\n",
    "        with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(blob, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    async def _perform_elder_content_check(\n",
    "        self,\n",
    "        request: StoryGenerationRequest,\n",
    "        image_bytes: bytes,\n",
    "    ) -> None:\n",
    "        return\n",
    "        cached = self._cache_get(\"elder_check\")\n",
    "        if cached:\n",
    "            if cached.get(\"is_restricted\"):\n",
    "                raise RestrictedContentDetected(cached.get(\"summary\"))\n",
    "            return\n",
    "        print(\"Performing elder content check...\")\n",
    "\n",
    "        img_bytes_url: str = self._image_to_data_url(image_bytes)\n",
    "\n",
    "        system = (\n",
    "            \"You are a concise content safety classifier.\"\n",
    "            \" Block explicit sexual content (nudity/acts/exploitation), graphic violence/gore,\"\n",
    "            \" sexualization of minors, or hateful/terrorist propaganda.\"\n",
    "            \" Allow 16+ content: mild romance/affection, non-graphic injuries, sports, everyday scenes.\"\n",
    "            \" Decisions must be grounded strictly in the visible image and the user's extra text.\"\n",
    "            \" Output a compact JSON object only.\"\n",
    "        )\n",
    "        user = (\n",
    "            \"Classify if the content should be restricted for under-18 viewers.\"\n",
    "            \" Consider the image and this extra text (may be empty):\\n\\n\"\n",
    "            f\"EXTRA_TEXT: {request.additional_context}\\n\\n\"\n",
    "            \" Keep summary one sentence, grounded in visible cues.\"\n",
    "        )\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": system}]},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": user},\n",
    "                    {\"type\": \"image_url\", \"image_url\": img_bytes_url},\n",
    "                ],\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        structured = (\n",
    "            ChatOllama(\n",
    "                model=self._vision_model_name,\n",
    "                base_url=self._ollama_url,\n",
    "                temperature=0,\n",
    "                num_ctx=8192\n",
    "            )\n",
    "            .with_structured_output(RestrictedContentResponse)\n",
    "        )\n",
    "\n",
    "        result = await structured.ainvoke(messages)\n",
    "        self._cache_set(\"elder_check\", result.model_dump())\n",
    "\n",
    "        if result.is_restricted:\n",
    "            raise RestrictedContentDetected(result.summary)\n",
    "\n",
    "    async def _get_image_insights(self, request: StoryGenerationRequest, image_bytes: bytes) -> ImageInsights:\n",
    "        cached = self._cache_get(\"image_insights\")\n",
    "        if cached:\n",
    "            return ImageInsights(**cached)\n",
    "        print(\"Getting image insights...\")\n",
    "\n",
    "        img_bytes_url: str = self._image_to_data_url(image_bytes)\n",
    "\n",
    "        system = (\n",
    "            \"You are a vision assistant extracting grounded story-building cues.\"\n",
    "            \" Be literal and faithful to the image; do not invent entities.\"\n",
    "            \" Output only JSON that matches the schema precisely.\"\n",
    "        )\n",
    "        user = (\n",
    "            \"Extract grounded insights for later story writing.\"\n",
    "            \" Keep items concise, no punctuation beyond commas where natural.\"\n",
    "            \" Take in consideration the user instructions for later story writing: \"\n",
    "            f\"```{request.additional_context}```\"\n",
    "        )\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": system}]},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": user},\n",
    "                    {\"type\": \"image_url\", \"image_url\": img_bytes_url},\n",
    "                ],\n",
    "            },\n",
    "        ]\n",
    "        structured = (\n",
    "            ChatOllama(\n",
    "                model=self._vision_model_name,\n",
    "                base_url=self._ollama_url,\n",
    "                temperature=0.3,\n",
    "                num_ctx=8192\n",
    "            )\n",
    "            .with_structured_output(ImageInsights)\n",
    "        )\n",
    "\n",
    "        result = cast(ImageInsights, await structured.ainvoke(messages))\n",
    "        self._cache_set(\"image_insights\", result.model_dump())\n",
    "        return result\n",
    "\n",
    "    async def _generate_story(\n",
    "        self,\n",
    "        request: StoryGenerationRequest,\n",
    "        insights: ImageInsights,\n",
    "    ) -> StoryGenerationResponse:\n",
    "        cached = self._cache_get(\"story\")\n",
    "        if cached:\n",
    "            return StoryGenerationResponse(**cached)\n",
    "        wpm = flavour_to_wpm[request.flavor]\n",
    "        minutes = 4.0\n",
    "        speech_margin = 0.92  # need this for pauses and extra effects\n",
    "        max_words = int(wpm * minutes * speech_margin)\n",
    "\n",
    "        tokens_to_predict = max(256, min(1024, int(max_words * 1.3)))\n",
    "\n",
    "        if request.eighting_plus_enabled:\n",
    "            content_guideline = (\n",
    "                \"18+ is enabled: mature themes are permitted. Do NOT depict minors, \"\n",
    "                \"illegal or non-consensual acts. Avoid pornographic detail; be tasteful.\"\n",
    "            )\n",
    "        else:\n",
    "            content_guideline = (\n",
    "                \"18+ is NOT enabled: content must be suitable for under-18. 16+ content is allowed. \"\n",
    "                \"No explicit sexual content; no graphic violence/gore.\"\n",
    "            )\n",
    "\n",
    "        system = (\n",
    "            \"You are a seasoned storyteller. Write vivid, coherent prose tailored to the requested flavor.\"\n",
    "            \" Keep language accessible and engaging. \"\n",
    "            \" Use natural rhythm for spoken delivery: short to medium sentences, varied cadence.\"\n",
    "            \" Format the story with clear paragraph breaks (one blank line between paragraphs).\"\n",
    "            \" For major shifts in scene or time, insert a line with '---' as a break.\"\n",
    "            f\"{content_guideline}\"\n",
    "            \" Output only the final story text; do not include a title or any commentary.\"\n",
    "        )\n",
    "        flavor_line = f\"Flavor: {request.flavor.value}.\"\n",
    "        context_line = f\"Additional instructions for the story: {request.additional_context}\"\n",
    "        insight_brief = (\n",
    "            \"Use the following details as inspiration for your story, but feel free to creatively expand, add new elements, or imagine additional context to make the story more engaging and vivid:\\n\"\n",
    "            f\"- Story Title: {insights.title}\\n\"\n",
    "            f\"- Caption: {insights.caption}\\n\"\n",
    "            f\"- Subjects: {', '.join(insights.subjects)}\\n\"\n",
    "            f\"- Setting: {insights.setting}\\n\"\n",
    "            f\"- Actions: {', '.join(insights.actions)}\\n\"\n",
    "            f\"- Mood: {', '.join(insights.mood)}\\n\"\n",
    "            f\"- Possible hooks (may be ignored): {', '.join(insights.hooks)}\\n\"\n",
    "        )\n",
    "        user = (\n",
    "            f\"Write a story. The story text should be medium-large: at least 300 words, but less than the {max_words} words.\\n\"\n",
    "            f\"{flavor_line}\\n\\n\"\n",
    "            f\"{insight_brief}\\n\"\n",
    "            f\"{context_line}\"\n",
    "        )\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": system}]},\n",
    "            {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": user}]},\n",
    "        ]\n",
    "\n",
    "        structured = (\n",
    "            ChatOllama(\n",
    "                model=self._txt_model_name,  # lightweight local model; VLM can do text-only\n",
    "                base_url=self._ollama_url,\n",
    "                temperature=1.2,\n",
    "                num_ctx=8192,\n",
    "                num_predict=tokens_to_predict,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        story_text = (await structured.ainvoke(messages)).content\n",
    "        # Use the title from image insights, not from the model output\n",
    "        result = StoryGenerationResponse(\n",
    "            title=insights.title, \n",
    "            text=story_text,\n",
    "        )\n",
    "        self._cache_set(\"story\", result.model_dump())\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def _image_to_data_url(image_bytes: bytes, mime: str = \"image/jpeg\") -> str:\n",
    "        b64 = base64.b64encode(image_bytes).decode(\"ascii\")\n",
    "        return f\"data:{mime};base64,{b64}\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def _convert_image_to_jpeg(image_bytes: bytes) -> bytes:\n",
    "        with BytesIO(image_bytes) as input_buffer:\n",
    "            image = Image.open(input_buffer).convert(\"RGB\")\n",
    "\n",
    "            with BytesIO() as output_buffer:\n",
    "                image.save(output_buffer, format=\"JPEG\", quality=92, subsampling=2, optimize=True, progressive=False)\n",
    "                return output_buffer.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58eb171",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de5b51a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ResponseError",
     "evalue": "model \"qwen2.5:7b\" not found, try pulling it first (status code: 404)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResponseError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 56\u001b[39m\n\u001b[32m     52\u001b[39m     pred_path.write_text(json.dumps(blob, ensure_ascii=\u001b[38;5;28;01mFalse\u001b[39;00m, indent=\u001b[32m2\u001b[39m), encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     53\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSaved \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(results)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m predictions to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpred_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m run_all()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mrun_all\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     37\u001b[39m data = json.loads(requests_path.read_text(encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     38\u001b[39m tasks = [run_one(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m data[:\u001b[32m2\u001b[39m]]\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m results = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*tasks)\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# also persist merged results to predictions.json for convenience\u001b[39;00m\n\u001b[32m     41\u001b[39m pred_path = Path(\u001b[33m\"\u001b[39m\u001b[33mdata/predictions.json\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36maretry.<locals>.wrap.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(times):\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fn(*args, **kwargs)\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m     16\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m i == times - \u001b[32m1\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mrun_one\u001b[39m\u001b[34m(rec)\u001b[39m\n\u001b[32m     30\u001b[39m image_bytes = (images_root / p.name).read_bytes()\n\u001b[32m     31\u001b[39m req = StoryGenerationRequest(**rec[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m])  \u001b[38;5;66;03m# pydantic handles aliases\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m gen.generate(req, image_bytes, cache_name=cache_name)\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cache_name, result.model_dump()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mStoryGenerator.generate\u001b[39m\u001b[34m(self, request, image_bytes, cache_name)\u001b[39m\n\u001b[32m     38\u001b[39m t_insights = perf_counter() - t2\n\u001b[32m     40\u001b[39m t3 = perf_counter()\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._generate_story(request, insights)\n\u001b[32m     42\u001b[39m t_story = perf_counter() - t3\n\u001b[32m     44\u001b[39m total = perf_counter() - t0\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 267\u001b[39m, in \u001b[36mStoryGenerator._generate_story\u001b[39m\u001b[34m(self, request, insights)\u001b[39m\n\u001b[32m    252\u001b[39m messages = [\n\u001b[32m    253\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: [{\u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: system}]},\n\u001b[32m    254\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: [{\u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: user}]},\n\u001b[32m    255\u001b[39m ]\n\u001b[32m    257\u001b[39m structured = (\n\u001b[32m    258\u001b[39m     ChatOllama(\n\u001b[32m    259\u001b[39m         model=\u001b[38;5;28mself\u001b[39m._txt_model_name,  \u001b[38;5;66;03m# lightweight local model; VLM can do text-only\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    264\u001b[39m     )\n\u001b[32m    265\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m story_text = (\u001b[38;5;28;01mawait\u001b[39;00m structured.ainvoke(messages)).content\n\u001b[32m    268\u001b[39m \u001b[38;5;66;03m# Use the title from image insights, not from the model output\u001b[39;00m\n\u001b[32m    269\u001b[39m result = StoryGenerationResponse(\n\u001b[32m    270\u001b[39m     title=insights.title, \n\u001b[32m    271\u001b[39m     text=story_text,\n\u001b[32m    272\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/WORK/envs/storytailer/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:415\u001b[39m, in \u001b[36mBaseChatModel.ainvoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    405\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m    407\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    412\u001b[39m     **kwargs: Any,\n\u001b[32m    413\u001b[39m ) -> BaseMessage:\n\u001b[32m    414\u001b[39m     config = ensure_config(config)\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     llm_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate_prompt(\n\u001b[32m    416\u001b[39m         [\u001b[38;5;28mself\u001b[39m._convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[32m    417\u001b[39m         stop=stop,\n\u001b[32m    418\u001b[39m         callbacks=config.get(\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    419\u001b[39m         tags=config.get(\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    420\u001b[39m         metadata=config.get(\u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    421\u001b[39m         run_name=config.get(\u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    422\u001b[39m         run_id=config.pop(\u001b[33m\"\u001b[39m\u001b[33mrun_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    423\u001b[39m         **kwargs,\n\u001b[32m    424\u001b[39m     )\n\u001b[32m    425\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m, llm_result.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/WORK/envs/storytailer/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1030\u001b[39m, in \u001b[36mBaseChatModel.agenerate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1021\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1022\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34magenerate_prompt\u001b[39m(\n\u001b[32m   1023\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1027\u001b[39m     **kwargs: Any,\n\u001b[32m   1028\u001b[39m ) -> LLMResult:\n\u001b[32m   1029\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1030\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate(\n\u001b[32m   1031\u001b[39m         prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n\u001b[32m   1032\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/WORK/envs/storytailer/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:988\u001b[39m, in \u001b[36mBaseChatModel.agenerate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    975\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[32m    976\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    977\u001b[39m             *[\n\u001b[32m    978\u001b[39m                 run_manager.on_llm_end(\n\u001b[32m   (...)\u001b[39m\u001b[32m    986\u001b[39m             ]\n\u001b[32m    987\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m988\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions[\u001b[32m0\u001b[39m]\n\u001b[32m    989\u001b[39m flattened_outputs = [\n\u001b[32m    990\u001b[39m     LLMResult(generations=[res.generations], llm_output=res.llm_output)  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m    991\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[32m    992\u001b[39m ]\n\u001b[32m    993\u001b[39m llm_output = \u001b[38;5;28mself\u001b[39m._combine_llm_outputs([res.llm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/WORK/envs/storytailer/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1158\u001b[39m, in \u001b[36mBaseChatModel._agenerate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1156\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1157\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._agenerate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1158\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(\n\u001b[32m   1159\u001b[39m         messages, stop=stop, run_manager=run_manager, **kwargs\n\u001b[32m   1160\u001b[39m     )\n\u001b[32m   1161\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1162\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/WORK/envs/storytailer/lib/python3.12/site-packages/langchain_ollama/chat_models.py:996\u001b[39m, in \u001b[36mChatOllama._agenerate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_agenerate\u001b[39m(\n\u001b[32m    990\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    991\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m    994\u001b[39m     **kwargs: Any,\n\u001b[32m    995\u001b[39m ) -> ChatResult:\n\u001b[32m--> \u001b[39m\u001b[32m996\u001b[39m     final_chunk = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._achat_stream_with_aggregation(\n\u001b[32m    997\u001b[39m         messages, stop, run_manager, verbose=\u001b[38;5;28mself\u001b[39m.verbose, **kwargs\n\u001b[32m    998\u001b[39m     )\n\u001b[32m    999\u001b[39m     generation_info = final_chunk.generation_info\n\u001b[32m   1000\u001b[39m     chat_generation = ChatGeneration(\n\u001b[32m   1001\u001b[39m         message=AIMessage(\n\u001b[32m   1002\u001b[39m             content=final_chunk.text,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1007\u001b[39m         generation_info=generation_info,\n\u001b[32m   1008\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/WORK/envs/storytailer/lib/python3.12/site-packages/langchain_ollama/chat_models.py:783\u001b[39m, in \u001b[36mChatOllama._achat_stream_with_aggregation\u001b[39m\u001b[34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[39m\n\u001b[32m    774\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_achat_stream_with_aggregation\u001b[39m(\n\u001b[32m    775\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    776\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m    780\u001b[39m     **kwargs: Any,\n\u001b[32m    781\u001b[39m ) -> ChatGenerationChunk:\n\u001b[32m    782\u001b[39m     final_chunk = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m783\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._aiterate_over_stream(messages, stop, **kwargs):\n\u001b[32m    784\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m final_chunk \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    785\u001b[39m             final_chunk = chunk\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/WORK/envs/storytailer/lib/python3.12/site-packages/langchain_ollama/chat_models.py:920\u001b[39m, in \u001b[36mChatOllama._aiterate_over_stream\u001b[39m\u001b[34m(self, messages, stop, **kwargs)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_aiterate_over_stream\u001b[39m(\n\u001b[32m    914\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    915\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m    916\u001b[39m     stop: Optional[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    917\u001b[39m     **kwargs: Any,\n\u001b[32m    918\u001b[39m ) -> AsyncIterator[ChatGenerationChunk]:\n\u001b[32m    919\u001b[39m     reasoning = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mreasoning\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m.reasoning)\n\u001b[32m--> \u001b[39m\u001b[32m920\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._acreate_chat_stream(messages, stop, **kwargs):\n\u001b[32m    921\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(stream_resp, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    922\u001b[39m             content = (\n\u001b[32m    923\u001b[39m                 stream_resp[\u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    924\u001b[39m                 \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m stream_resp \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m stream_resp[\u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    925\u001b[39m                 \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    926\u001b[39m             )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/WORK/envs/storytailer/lib/python3.12/site-packages/langchain_ollama/chat_models.py:728\u001b[39m, in \u001b[36mChatOllama._acreate_chat_stream\u001b[39m\u001b[34m(self, messages, stop, **kwargs)\u001b[39m\n\u001b[32m    725\u001b[39m chat_params = \u001b[38;5;28mself\u001b[39m._chat_params(messages, stop, **kwargs)\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chat_params[\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m728\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._async_client.chat(**chat_params):\n\u001b[32m    729\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m part\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/WORK/envs/storytailer/lib/python3.12/site-packages/ollama/_client.py:682\u001b[39m, in \u001b[36mAsyncClient._request.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    680\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    681\u001b[39m   \u001b[38;5;28;01mawait\u001b[39;00m e.response.aread()\n\u001b[32m--> \u001b[39m\u001b[32m682\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e.response.text, e.response.status_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    684\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m r.aiter_lines():\n\u001b[32m    685\u001b[39m   part = json.loads(line)\n",
      "\u001b[31mResponseError\u001b[39m: model \"qwen2.5:7b\" not found, try pulling it first (status code: 404)"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import wraps\n",
    "\n",
    "\n",
    "def aretry(times: int = 3):\n",
    "    def wrap(fn):\n",
    "        @wraps(fn)\n",
    "        async def inner(*args, **kwargs):\n",
    "            for i in range(times):\n",
    "                try:\n",
    "                    return await fn(*args, **kwargs)\n",
    "                except Exception:\n",
    "                    if i == times - 1:\n",
    "                        raise\n",
    "        return inner\n",
    "    return wrap\n",
    "\n",
    "requests_path = Path(\"data/image-generation-requests.json\")\n",
    "images_root = Path(\"data/images\")\n",
    "\n",
    "@aretry(3)\n",
    "async def run_one(rec: dict):\n",
    "    gen = StoryGenerator()\n",
    "\n",
    "    p = Path(rec[\"imagePath\"])  # already relative\n",
    "    cache_name = p.stem\n",
    "    image_bytes = (images_root / p.name).read_bytes()\n",
    "    req = StoryGenerationRequest(**rec[\"request\"])  # pydantic handles aliases\n",
    "    result = await gen.generate(req, image_bytes, cache_name=cache_name)\n",
    "\n",
    "    return cache_name, result.model_dump()\n",
    "\n",
    "async def run_all():\n",
    "    data = json.loads(requests_path.read_text(encoding=\"utf-8\"))\n",
    "    tasks = [run_one(r) for r in data[:2]]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    # also persist merged results to predictions.json for convenience\n",
    "    pred_path = Path(\"data/predictions.json\")\n",
    "    blob = {}\n",
    "    if pred_path.exists():\n",
    "        try:\n",
    "            blob = json.loads(pred_path.read_text(encoding=\"utf-8\"))\n",
    "        except Exception:\n",
    "            blob = {}\n",
    "\n",
    "    for k, v in results:\n",
    "        blob[k] = v\n",
    "\n",
    "    pred_path.write_text(json.dumps(blob, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    print(f\"Saved {len(results)} predictions to {pred_path}\")\n",
    "\n",
    "\n",
    "await run_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71fa6e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "storytailer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
